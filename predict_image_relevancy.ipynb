{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81630b75",
   "metadata": {},
   "source": [
    "### PROBLEM SOLVING SUMMARY\n",
    "\n",
    "##### PROMBLEM STATEMENT\n",
    "\n",
    "To predict if an image is relevant or not given a text query\n",
    "\n",
    "##### DATA GIVEN\n",
    "\n",
    "1. “id” : unique identifier for an image\n",
    "2. “query” : text query, which is used to determine the relevance of an image\n",
    "3. “url_page”: webpage where the image is found\n",
    "4. “image_url” : the source image url of the image, this is the url for the image itself\n",
    "5. “title”: title of the “url_page”\n",
    "6. “alt”: alternate text for the image\n",
    "7. “is_relevance”: 1 if image is relevant to the query, 0 otherwise\n",
    "\n",
    "and other html tag fields surronding the image\n",
    "\n",
    "ps: \"“image_url” wasn't present but understood \"src\" is actually the url of the image and hence considered it as image url\n",
    "\n",
    "###### APPROACH \n",
    "\n",
    "1. Pre-processed and cleaned the data set \n",
    "2. Built a base-line model and measure the accuracy\n",
    "3. Further processing / techniques to improvise the model \n",
    "\n",
    "PS : Only the final steps may be shown here in the notebook. I didn't show all of the preprocessing steps or model evaluation steps to avoid confusion but I have explained in detail .\n",
    "\n",
    "######  EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "Below are the text processing done on the data\n",
    "\n",
    "- All the fiels were considered initially that can be used in the model\n",
    "- Total number of records present in the data : 607844\n",
    "- Total number of features : 23\n",
    "- No duplicates found in the data\n",
    "\n",
    "- Feature selection & Engineering\n",
    "    - 9 features had more than 80% of missing values and were dropped (Features mentioned in EDA)\n",
    "    - After heuristicallly examining the data and doing some basic stats, text in 6 of the features are not relevant to the image . Hence 6 features were dropped\n",
    "    - 'url_page' showed a more generic description of the image compared to the 'src' . And hence it was dropped too\n",
    "    - 'text' feature did show a lot of relavance to the image and hence it was retained. \n",
    "    - A new feature 'combined_text' feature was formed to concatenate all the text features together\n",
    "    - Initally , only 'query', 'src', 'title', 'alt' were concatenated together but later after including 'text' the model performed better . A total of 5 features were selected from the data and combined as an input for the model\n",
    "    \n",
    "- Text preprocessing\n",
    "    -  Conversion of text to lowercase, removal of punctuations, extra characters, digits , single characters and extra whitespaces were done on the data\n",
    "    - Since it's a classification problem, there weren't any need for stopwords and hence it wasremoved \n",
    "    - Lemmatization function is written but it took time to run and hence included in 'Next steps\"\n",
    "    \n",
    "Issue: Initially the features were combined and preprocessed together but it took a lot of time to process\n",
    "Resolve : So analysed the word count of each of the 5 features and found 'text' and 'alt' had maximum number of word count. So processed them individually and concatenated later\n",
    "\n",
    "There  is a 35% percent decrease in word count after text preprocessing steps.\n",
    "\n",
    "- Class imbalance\n",
    "    - The class is higly imbalanced. 96% of the data pointed that the image is irrelavant (class : 0) while only the rest 4% had said it was relevant (class : 1) \n",
    "    \n",
    "- Initally TF-IDF was applied to vectorize the data\n",
    "\n",
    "###### BASELINE MODEL\n",
    "\n",
    "- A logistic regression model was built after the above text processing steps and roc_auc_score was 53.9%\n",
    "- Alternatively, Naivebase classifier and XG boost were also built \n",
    "     - Naive Bayes score is : 50.0%\n",
    "     - XG boost score is : 53.2%\n",
    " \n",
    "PS: since the accuracy metric was given already , only used roc_auc_score to measure model performance.\n",
    "\n",
    "- Resolving strategies : Two main things were handled to improve the accuracy \n",
    "    - Embeddings :  Trying to see if the semantic meaning of the words in the text can be used . Also if the pre and post words in the sentences makes adds meaning to the data\n",
    "    - Handling model imbalance\n",
    "\n",
    "###### IMPROVISATIONS AND ISSUES\n",
    "\n",
    "- Below are the improvising steps that were handled (included those that didn't work out too)\n",
    "- Working with the data imbalance   \n",
    "   -  Tried StratiefiedShufllesplit , to make sure there are equal distributions on the train and set and didn't show any big difference in the model\n",
    "    - Parameter tuned LR model using class_weight set to '{0:1,1:5}' , giving more weightage to 1 class. LR gave out an accuracy of 58%\n",
    "    - Included 'text' field in the prediction , LR's performance rose up to 59.1%\n",
    "    - Parameter tuned LR model using class_weight set to 'balanced' , giving more weightage to 1 class. LR gave out an accuracy of 66.9%. There is chance of overfitting and hence tried handling imbalance in boosting classifiers as well\n",
    "- Working with features\n",
    "    - Tried to extract Semantinc meaning of the text in the data and hence word2vec was applied inplace of tf-idf for vectorization. But didn't show any major improvement but only decreased the model performance.\n",
    "    - Since BERT helps machine learn excellent reperesentations of text with respect to text context, tried to use BERT. But I was't able to install 'tensorflow-text' library in my machine(macbook pro M1 chip) and when i degraded the the tensorflow version to 2.9 , it popped errors to import the module. Spent more than half day trying to set up the configuration and hence dropped this idea\n",
    "    - The next thing I tried was using LSTM , considering the nature of the  image decription that does have some continous text in it. I wrote the code for executing the LSTM but it ran for more than 8 hrs and hence dropped it as well considering the time constraint . \n",
    "    - The deep learning models took more than a day for configuration setup and running\n",
    "    - Finally , Parameter tuned XGboost scale_pos_weight parameter with tf-idf to the optimum level after running it on multiple weights and it performed well with an accuracy of 74.3%\n",
    "    \n",
    "   \n",
    "###### RESULTS\n",
    "\n",
    "- Selected XGBoost classifier among all the models for prediction which gave roc_auc_score of 74.3%\n",
    "\n",
    "\n",
    "###### NEXT STEPS\n",
    "Considering the time for running and exploring, I would consider the following ideas as my next steps\n",
    "\n",
    "- Try to add more features (extract the overall text in the url page and see if it is relevant to the image). Feature engineering more might actually help building the model accuracy with out adjusting the class balance\n",
    "- Perform dimensionality reduction and capture the relationship between features. \n",
    "- Definitely if given access to the data, add in more data to handle class imbalance . Adding in more data would defintely help avoid overfitting\n",
    "- Configure to run BERT and LSTM to make sure the better semantic meaning of the text in the data is captured.\n",
    "- Hypyerparamater optimization on the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606dfef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#for handling imbalance in the dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f29969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the libraries for text processing\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a50e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional settings for exploratory analysis\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c486a2",
   "metadata": {},
   "source": [
    "###  INPUT OUTPUT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "827ebdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(file):\n",
    "    \"\"\"\n",
    "    Reads input file into pandas dataframe\n",
    "    file : path of the file\n",
    "    return : pandas dataframe\n",
    "    \"\"\"\n",
    "    input_file = pd.read_feather(file)\n",
    "    return input_file\n",
    "\n",
    "def write_to_csv(data, columns, filename):\n",
    "    \"\"\"\n",
    "    write pandas dataframe to csv\n",
    "    data: dataframe name\n",
    "    columns: columns to be written to csv\n",
    "    filename : name of the file\n",
    "    \"\"\"\n",
    "    data[columns].to_csv(filename, index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4601dc",
   "metadata": {},
   "source": [
    "### GENERAL PROCESSING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7215027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(data,list_of_cols):\n",
    "    \"\"\"\n",
    "    drops the listed columns\n",
    "    data : pandas dataframe\n",
    "    list_of_cols : column list\n",
    "    return : pandadataframe with dropped columns\n",
    "    \"\"\"\n",
    "    data.drop(columns=list_of_cols, inplace = True)\n",
    "    data\n",
    "    \n",
    "    \n",
    "def convert_to_str(data, column_name):\n",
    "    \"\"\"\n",
    "    convert a column to string type\n",
    "    data : pandas dataframe\n",
    "    column_name : name_of_column\n",
    "    return : pandadataframe \n",
    "    \"\"\"\n",
    "    data[column_name] = data[column_name].astype(str)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25240698",
   "metadata": {},
   "source": [
    "### TEXT PROCESSING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07827caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_to_str(input_file, column_name):\n",
    "    \"\"\"\n",
    "    converts the feature to str\n",
    "    input_file : pandas dataframe\n",
    "    column_name : feature name\n",
    "    return : connverted feature name as str type\n",
    "    \"\"\"\n",
    "    input_file[column_name] = input_file[column_name].astype(str)\n",
    "    return input_file\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    It does the following in order:\n",
    "    1.Converts text to lowercase\n",
    "    2.Strips of leading, trailing white spaces\n",
    "    3.Removes special characters\n",
    "    4.Removes punctuations\n",
    "    5.Removes extra white spaces\n",
    "    6.Removes digits\n",
    "    7.Removes any other whitespaces\n",
    "    8.Remove other digit characters\n",
    "    9.Remove single characters\n",
    "    text: text to process\n",
    "    return: clean text\n",
    "    \"\"\"\n",
    "    text = text.lower() \n",
    "    text = text.strip()  \n",
    "    text = re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'(^| ).( |$)',' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    removes stopwords from the text\n",
    "    text : text to process\n",
    "    return: text with removed stopwords\n",
    "    \"\"\"\n",
    "    a= [word for word in text.split() if word not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    Reduce the text to it's orginal form depending on it's tag\n",
    "    text : text to process\n",
    "    return : lemmatized text\n",
    "    \"\"\"\n",
    "    \n",
    "    wn = WordNetLemmatizer()\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    \n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(text))\n",
    "    a=[wn.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] \n",
    "    return \" \".join(a)\n",
    "\n",
    "\n",
    "def all_preprocess(string):\n",
    "    \"\"\"\n",
    "    Combines all the text preprocessing steps\n",
    "    string: text to process\n",
    "    return : clean text \n",
    "    \"\"\"\n",
    "    return remove_stopwords(preprocess(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3afd46c",
   "metadata": {},
   "source": [
    "### MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aafd4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #read in the input data\n",
    "    data = read_input('data/train.feather')\n",
    "    test_data = read_input('data/test.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede28d40",
   "metadata": {},
   "source": [
    "## Exploratory data analysis - Basic statistics , Feature Engineering & Feature Selection\n",
    "\n",
    "###### Total number of rows : 607844\n",
    "###### No duplicates\n",
    "###### Number of features : 23\n",
    "###### 733 distinct queries\n",
    "###### Features like 'crossorigin', 'ismap', 'loading', 'longdesc',' referrerpolicy', 'sizes', 'srcset', 'style', have more 80% of missing values (shown as below) in them and doesn't make sense to include in them in the model. Hence all of them are dropped \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6fc3cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query              0.000000\n",
       "url_page           0.000000\n",
       "title              0.000000\n",
       "source             0.000000\n",
       "alt               15.009114\n",
       "src                0.000000\n",
       "crossorigin       99.947684\n",
       "height            59.597693\n",
       "ismap             99.998519\n",
       "loading           80.367496\n",
       "longdesc          99.758326\n",
       "referrerpolicy    99.946697\n",
       "sizes             89.308605\n",
       "srcset            82.591257\n",
       "width             57.822895\n",
       "class             49.008298\n",
       "style             87.183718\n",
       "tree_path          0.000000\n",
       "deg               30.277999\n",
       "text_tag          30.277999\n",
       "text              30.277999\n",
       "id                 0.000000\n",
       "is_relevant        0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum() * 100 / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e66c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "drop_columns(data, ['crossorigin', 'height', 'ismap', 'loading', 'longdesc', 'referrerpolicy', 'sizes', 'srcset','style'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d28d6f",
   "metadata": {},
   "source": [
    "###### Features like 'width', 'class', 'deg', 'text_tag', 'tree_path' don't have any real information that can add value to the text description we want to build and hence are removed on heuristic basics\n",
    "\n",
    "###### Text in 'url_page' is more specific compared to the 'source' feature and might only create more deviation from the prediction . Hence is removed\n",
    "\n",
    "##### Feature 'text' add more descriptions to the exact image and hence it is retained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3072f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns based on heuristics\n",
    "drop_columns(data, ['width', 'class', 'deg', 'text_tag', 'tree_path', 'source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27c8c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    586007\n",
      "1     21837\n",
      "Name: is_relevant, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='is_relevant'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Target feature distribution\n",
    "print(data.is_relevant.value_counts())\n",
    "\n",
    "sns.barplot(x=data.is_relevant.value_counts().index, y=data.is_relevant.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b174106",
   "metadata": {},
   "source": [
    "#### data is imbalanced , there 96% of 0 compared to 1 , hence balancing techniques need to be applied while building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46bfab6",
   "metadata": {},
   "source": [
    "#### TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0820bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing text \n",
    "\n",
    "#converting to str type for further processing\n",
    "convert_to_str(data, 'alt')\n",
    "convert_to_str(data, 'text')\n",
    "\n",
    "#preprocessing all text columns\n",
    "data['clean_query'] = data['query'].apply(lambda x: all_preprocess(x))\n",
    "data['clean_title'] = data['title'].apply(lambda x: all_preprocess(x))\n",
    "data['clean_src'] = data['src'].apply(lambda x: all_preprocess(x))\n",
    "data['clean_alt'] = data['alt'].apply(lambda x: all_preprocess(x))\n",
    "data['clean_text'] = data['text'].apply(lambda x: all_preprocess(x))\n",
    "# Merge all of the text columns to form one feature variable for prediction\n",
    "data['combined_text'] = data[['clean_query', 'clean_title', 'clean_alt', 'clean_src' ,'clean_text' ]].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414d1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined_text'] = data[['clean_query', 'clean_title', 'clean_alt', 'clean_src' ,'clean_text' ]].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf96b423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    607844.000000\n",
       "mean        120.058800\n",
       "std        1250.870275\n",
       "min           6.000000\n",
       "25%          22.000000\n",
       "50%          29.000000\n",
       "75%          41.000000\n",
       "max       76553.000000\n",
       "Name: combined_text, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word count\n",
    "data['combined_text'].apply(lambda x: len(str(x).split())).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f5880",
   "metadata": {},
   "source": [
    "##### TRAINING AND SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6fc70e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    468805\n",
      "1     17470\n",
      "Name: is_relevant, dtype: int64\n",
      "0    117202\n",
      "1      4367\n",
      "Name: is_relevant, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Input: \"combined_text\"\n",
    "# Target: \"is_relevant\"\n",
    "\n",
    "#Handling imbalance in the data \n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.20,\n",
    "                             random_state=0)\n",
    "\n",
    "for train, test in splitter.split(data[\"combined_text\"], data[\"is_relevant\"]):\n",
    "    X_train = data[\"combined_text\"].iloc[train]\n",
    "    y_train = data[\"is_relevant\"].iloc[train]\n",
    "    X_test = data[\"combined_text\"].iloc[test]\n",
    "    y_test = data[\"is_relevant\"].iloc[test]\n",
    "    \n",
    "    \n",
    "\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731a32b",
   "metadata": {},
   "source": [
    "##### VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "189a6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using tf-idf\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a276d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using word2vec\n",
    "\n",
    "data['clean_text_tok']=[nltk.word_tokenize(i) for i in data['combined_text']]\n",
    "w2v_model = gensim.models.Word2Vec(data['clean_text_tok'], vector_size=100, window=5, min_count=5, workers=4)\n",
    "w2v = dict(zip(w2v_model.wv.index_to_key, w2v_model.wv.vectors)) \n",
    "\n",
    "#embedding function to get the mean of all vectors\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    \"\"\"\n",
    "    Calculates the mean of all vectors of the words in a document\n",
    "    \"\"\"\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd7546dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the text\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train] \n",
    "X_test_tok= [nltk.word_tokenize(i) for i in X_test]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d1917ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying word2vec\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75015437",
   "metadata": {},
   "source": [
    "##### MODEL BUILDING\n",
    "\n",
    "#### LOGISTIC REGRESSION(TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "070de87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[116863    339]\n",
      " [  4013    354]]\n",
      "\n",
      " Roc Auc Score:  0.5390850361016941\n"
     ]
    }
   ],
   "source": [
    "# logistic regression model (Handling imbalanced dataset using class_weight)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "\n",
    "#fit the model\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "\n",
    "\n",
    "#Prediction\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    "\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "print (\"\\n Roc Auc Score: \", roc_auc_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68e248f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[106563  10639]\n",
      " [  2490   1877]]\n",
      "\n",
      " Roc Auc Score:  0.6695198082617667\n"
     ]
    }
   ],
   "source": [
    "# logistic regression model (Handling imbalanced dataset using class_weight)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2', class_weight = 'balanced')\n",
    "\n",
    "#fit the model\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "\n",
    "\n",
    "#Prediction\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    "\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "print (\"\\n Roc Auc Score: \", roc_auc_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc912c",
   "metadata": {},
   "source": [
    "#### NAIVE BAYES(TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5afd3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[117164     38]\n",
      " [  4361      6]]\n",
      "\n",
      " Roc Auc Score:  0.5005248571857528\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "\n",
    "\n",
    "#Prediction\n",
    "y_predict = nb_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = nb_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    "\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "print (\"\\n Roc Auc Score: \", roc_auc_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc82d8cd",
   "metadata": {},
   "source": [
    "#### XG BOOSTING (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdd9d725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[117113     89]\n",
      " [  4077    290]]\n",
      "\n",
      " Roc Auc Score:  0.5328238858929182\n"
     ]
    }
   ],
   "source": [
    "#XGB model\n",
    "xg_model = XGBClassifier()\n",
    "xg_model.fit(X_train_vectors_tfidf, y_train)\n",
    "\n",
    "\n",
    "# Creating the model on Training Data\n",
    "y_predict = xg_model.predict(X_test_vectors_tfidf)\n",
    "y_prob = xg_model.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    " \n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "print (\"\\n Roc Auc Score: \", roc_auc_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aee08c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when scale_pos_weight is : 1  Accuracy is : 0.5328238858929182\n",
      "when scale_pos_weight is : 10  Accuracy is : 0.6574680765331586\n",
      "when scale_pos_weight is : 25  Accuracy is : 0.7435707979186339\n",
      "when scale_pos_weight is : 50  Accuracy is : 0.7334029948438979\n",
      "when scale_pos_weight is : 75  Accuracy is : 0.7131189740984787\n",
      "when scale_pos_weight is : 99  Accuracy is : 0.7003729304777009\n",
      "when scale_pos_weight is : 100  Accuracy is : 0.7034619813100567\n",
      "when scale_pos_weight is : 1000  Accuracy is : 0.6627074733103928\n"
     ]
    }
   ],
   "source": [
    "#Handling imbalance data using XGboost - parameter tunning\n",
    "weights = [1, 10, 25, 50, 75, 99, 100, 1000]\n",
    "\n",
    "for weigh in weights:\n",
    "    #XGB model\n",
    "    xg_model = XGBClassifier(scale_pos_weight=weigh)\n",
    "    xg_model.fit(X_train_vectors_tfidf, y_train)\n",
    "\n",
    "\n",
    "    # Creating the model on Training Data\n",
    "    y_predict = xg_model.predict(X_test_vectors_tfidf)\n",
    "    y_prob = xg_model.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    "    print (\"when scale_pos_weight is :\",weigh,\" Accuracy is :\",roc_auc_score(y_test, y_predict))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a81aada7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[92896 24306]\n",
      " [ 1334  3033]]\n",
      "\n",
      " Roc Auc Score:  0.7435707979186339\n"
     ]
    }
   ],
   "source": [
    "#XGB model (TF-IDF) \n",
    "final_xgbmodel_tfidf = XGBClassifier(scale_pos_weight=25)\n",
    "final_xgbmodel_tfidf.fit(X_train_vectors_tfidf, y_train)\n",
    "\n",
    "\n",
    "# Creating the model on Training Data\n",
    "final_predict = final_xgbmodel_tfidf.predict(X_test_vectors_tfidf)\n",
    "final_prob = final_xgbmodel_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    " \n",
    "print('Confusion Matrix:',confusion_matrix(y_test, final_predict))\n",
    "print (\"\\n Roc Auc Score: \", roc_auc_score(y_test, final_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f84569",
   "metadata": {},
   "source": [
    "#### XGBOOST (WORD2VEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bc9078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[96038 21164]\n",
      " [ 1982  2385]]\n",
      "0.6827821963287667\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(scale_pos_weight=25)\n",
    "final_model.fit(X_train_vectors_w2v, y_train)\n",
    "\n",
    "\n",
    "# Creating the model on Training Data\n",
    "final_predict = final_model.predict(X_test_vectors_w2v)\n",
    "final_prob = final_model.predict_proba(X_test_vectors_w2v)[:,1]\n",
    " \n",
    "print('Confusion Matrix:',confusion_matrix(y_test, final_predict))\n",
    "\n",
    "print (roc_auc_score(y_test, final_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a0b45",
   "metadata": {},
   "source": [
    "#### LOGISTIC REGRESSION (WORD2VEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "208ae196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[73720 43482]\n",
      " [ 1439  2928]]\n",
      "\n",
      " Auc Score:  0.6497413371758112\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
    "lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2', class_weight = 'balanced')\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    " \n",
    "\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "print (\"\\n Auc Score: \", roc_auc_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53e979",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a2b5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af650129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Keras tokenizer\n",
    "num_words = 2000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data['combined_text'].values)\n",
    "\n",
    "# Pad the data \n",
    "X = tokenizer.texts_to_sequences(data['combined_text'].values)\n",
    "X = pad_sequences(X, maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f69e8778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 10:20:38.398888: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-28 10:20:38.399058: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2000, 128)         256000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 196)               254800    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 394       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build out our simple LSTM\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "# Model saving callback\n",
    "ckpt_callback = ModelCheckpoint('keras_model', \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='auto')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embed_dim, input_length = X.shape[1]))\n",
    "model.add(LSTM(lstm_out, recurrent_dropout=0.2, dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ce4ef06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(486275, 2000) (486275, 2)\n",
      "(121569, 2000) (121569, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['is_relevant']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42, stratify=Y)\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63228658",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs=8, batch_size=batch_size, validation_split=0.2, callbacks=[ckpt_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755ba60",
   "metadata": {},
   "source": [
    "##### The training takes more than 8 hours- so stopped it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc98c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = load_model('keras_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_lstm = lstm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "print (\"\\n Auc Score: \", roc_auc_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3091b9",
   "metadata": {},
   "source": [
    "#### PREDICTING ON UNSEEN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b6f33f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns(test_data, ['crossorigin', 'height', 'ismap', 'loading', 'longdesc', 'referrerpolicy', 'sizes', 'srcset','style', 'width', 'class', 'deg', 'text_tag', 'tree_path', 'source', 'url_page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3d8b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing text \n",
    "\n",
    "#converting to str type for further processing\n",
    "convert_to_str(test_data, 'alt')\n",
    "convert_to_str(test_data, 'text')\n",
    "\n",
    "#preprocessing all text columns\n",
    "test_data['clean_query'] = test_data['query'].apply(lambda x: all_preprocess(x))\n",
    "test_data['clean_title'] = test_data['title'].apply(lambda x: all_preprocess(x))\n",
    "test_data['clean_src'] = test_data['src'].apply(lambda x: all_preprocess(x))\n",
    "test_data['clean_alt'] = test_data['alt'].apply(lambda x: all_preprocess(x))\n",
    "test_data['clean_text'] = test_data['text'].apply(lambda x: all_preprocess(x))\n",
    "# Merge all of the text columns to form one feature variable for prediction\n",
    "test_data['combined_text'] = test_data[['clean_query', 'clean_title', 'clean_alt', 'clean_src' ,'clean_text' ]].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "322e09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing the input\n",
    "test_data_vect = tfidf_vectorizer.transform(test_data['combined_text']) \n",
    "\n",
    "# Predicting on on the test data \n",
    "Y_test_data_predict = final_xgbmodel_tfidf.predict(test_data_vect)\n",
    "y_test_data_prob = lr_tfidf.predict_proba(test_data_vect)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07ebd33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the input to csv file\n",
    "test_data['predict_prob']= y_test_data_prob\n",
    "test_data['is_relevant']= Y_test_data_predict\n",
    "final=test_data[['id','combined_text','is_relevant', 'predict_prob']].reset_index(drop=True)\n",
    "write_to_csv(test_data, ['id', 'is_relevant'], \"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
